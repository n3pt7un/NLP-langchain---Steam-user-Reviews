{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG application for game research \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# LangSmith imports for tracing\n",
    "from langsmith import traceable\n",
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as rest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract game name from query - keeping for backward compatibility\n",
    "def extract_game_name(query: str, games_csv_path: str = \"games.csv\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract game name from a user query by checking against a CSV file of game names.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's query text\n",
    "        games_csv_path: Path to CSV file containing game names\n",
    "        \n",
    "    Returns:\n",
    "        The extracted game name or None if no game found\n",
    "    \"\"\"\n",
    "    # Check if CSV file exists\n",
    "    if not os.path.exists(games_csv_path):\n",
    "        print(f\"Warning: Games CSV file {games_csv_path} not found.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load game names from CSV\n",
    "        games_df = pd.read_csv(games_csv_path, header=None)\n",
    "        game_names = games_df[0].tolist()  # Assuming the game names are in the first column\n",
    "        \n",
    "        # Normalize query for matching\n",
    "        normalized_query = query.lower()\n",
    "        \n",
    "        # Sort game names by length (descending) to prioritize longer matches\n",
    "        for game in sorted(game_names, key=len, reverse=True):\n",
    "            # Check for case-insensitive match\n",
    "            if game.lower() in normalized_query:\n",
    "                return game\n",
    "        \n",
    "        # Enhanced detection with patterns\n",
    "        patterns = [\n",
    "            r\"for\\s+(.+?)(?:\\s+game|\\s+reviews|\\s+in|\\?|$)\",  # \"for [game]\"\n",
    "            r\"about\\s+(.+?)(?:\\s+game|\\s+reviews|\\?|$)\",      # \"about [game]\"\n",
    "            r\"in\\s+(.+?)(?:\\s+game|\\s+reviews|\\?|$)\",         # \"in [game]\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.search(pattern, normalized_query)\n",
    "            if matches:\n",
    "                potential_game = matches.group(1).strip()\n",
    "                # Find the closest match in our game list\n",
    "                for game in game_names:\n",
    "                    if potential_game in game.lower() or game.lower() in potential_game:\n",
    "                        return game\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting game name: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@traceable(name=\"setup_rag_retriever\")\n",
    "def setup_rag_retriever(\n",
    "    collection_name: str = \"steam_reviews\",\n",
    "    openai_api_key: str = None,\n",
    "    embedding_model: str = \"text-embedding-3-small\",\n",
    "    search_type: str = \"similarity\",\n",
    "    k: int = 4\n",
    "):\n",
    "    \"\"\"\n",
    "    Set up a langchain retriever from an existing Qdrant collection with tracing.\n",
    "    No game filtering - will format game name in the documents during context preparation.\n",
    "    \"\"\"\n",
    "    if openai_api_key is None:\n",
    "        openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "        \n",
    "    # Setup embeddings\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=embedding_model,\n",
    "        openai_api_key=openai_api_key\n",
    "    )\n",
    "    \n",
    "    # Connect to existing Qdrant collection\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "    \n",
    "    # Create vector store without game filtering\n",
    "    vector_store = Qdrant(\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "    \n",
    "    # Create retriever with specified search parameters\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=search_type,\n",
    "        search_kwargs={\"k\": k}\n",
    "    )\n",
    "    \n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize callback manager for tracing\n",
    "def get_tracer_callback_manager():\n",
    "    tracer = LangChainTracer(project_name=os.environ.get(\"LANGCHAIN_PROJECT\", \"steam-reviews-rag\"))\n",
    "    return CallbackManager([tracer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom function to format documents\n",
    "def format_documents(docs):\n",
    "    \"\"\"\n",
    "    Format each document to include game name from metadata.\n",
    "    Format: \"Review for {game_name}: {review_content}\"\n",
    "    \"\"\"\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        game_name = doc.metadata.get(\"game_name\", \"Unknown Game\")\n",
    "        formatted_text = f\"Review for \\\"{game_name}\\\": {doc.page_content}\"\n",
    "        formatted_docs.append(formatted_text)\n",
    "    return \"\\n\\n\".join(formatted_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Updated RAG system setup without game filtering\n",
    "@traceable(name=\"setup_rag_system\")\n",
    "def setup_rag_system(collection_name=\"steam_reviews\", k=25, model=\"gpt-4o-mini\", temperature=0):\n",
    "    \"\"\"\n",
    "    Set up RAG system that formats review context to include game name from metadata.\n",
    "    \"\"\"\n",
    "    # Set up client\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "    \n",
    "    # Set up retriever without game filtering\n",
    "    retriever = setup_rag_retriever(\n",
    "        collection_name=collection_name, \n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    # Set up prompt template\n",
    "    template = \"\"\"You are an AI assistant helping game developers improve their games based on Steam reviews.\n",
    "    To answer the question, you will be given a list of reviews mentioning the game requested. The reviews are formatted as follows:\n",
    "    \"Review for [game_name]: [review_content]\"\n",
    "    Always check that the game name is correct for the reviews considered. If the game requested does not match the game passed in one \n",
    "    or more reviews then ignore the reviews that do not match the game name, unless the content of the review is relevant to the question.\n",
    "    If you don't know the answer, just say that you don't know. If the question is not related to the game, just say that you don't know.\n",
    "    If the question is about a game not explicitly present in the context, just say that you don't know.\n",
    "    Always verify that the game specified in the question is present in the context before answering.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    PROMPT = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    # Set up LLM\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=temperature, \n",
    "        model=model\n",
    "    )\n",
    "    \n",
    "    # Class to customize document formatting for RAG\n",
    "    class CustomFormatRAG:\n",
    "        def __init__(self, llm, retriever, prompt):\n",
    "            self.llm = llm\n",
    "            self.retriever = retriever\n",
    "            self.prompt = prompt\n",
    "        \n",
    "        def invoke(self, params):\n",
    "            query = params.get(\"query\", \"\")\n",
    "            \n",
    "            # Execute retrieval to get documents\n",
    "            docs = self.retriever.get_relevant_documents(query)\n",
    "            \n",
    "            # Format documents to include game name\n",
    "            formatted_context = format_documents(docs)\n",
    "            \n",
    "            # Execute the query with the LLM\n",
    "            llm_response = self.llm.invoke(\n",
    "                self.prompt.format(\n",
    "                    context=formatted_context,\n",
    "                    question=query\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Format the response to match the expected structure\n",
    "            result = {\n",
    "                \"result\": llm_response.content,\n",
    "                \"source_documents\": docs\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "    \n",
    "    # Create and return our custom RAG implementation\n",
    "    return CustomFormatRAG(llm, retriever, PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to run RAG query\n",
    "@traceable(name=\"run_rag_query\")\n",
    "def run_rag_query(qa_system, query):\n",
    "    \"\"\"\n",
    "    Run a RAG query with comprehensive tracing for performance analysis\n",
    "    \"\"\"\n",
    "    # Execute query using invoke method\n",
    "    result = qa_system.invoke({\"query\": query})\n",
    "    \n",
    "    # Calculate result metrics\n",
    "    run_metrics = {\n",
    "        \"query\": query,\n",
    "        \"result_length\": len(result.get(\"result\", \"\")),\n",
    "        \"num_source_docs\": len(result.get(\"source_documents\", [])),\n",
    "    }\n",
    "    \n",
    "    return result, run_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: The most common complaint about Counter-Strike: Global Offensive (CS:GO) in comparison to older Counter-Strike games is the lack of innovation and significant changes to gameplay. Many reviews express that CS:GO feels like a rehash of its predecessors with minimal improvements, leading to a stale experience for longtime fans. Additionally, issues such as unbalanced gameplay, inconsistent hit registration, and a toxic community are frequently mentioned, contributing to a sense of frustration among players. The matchmaking system is also criticized for being inefficient and unbalanced, further detracting from the overall experience.\n",
      "Metrics: {'query': 'What is the most common complaint about Counter-Strike Global Offensive in comparison to older Counter-Strike games?', 'result_length': 638, 'num_source_docs': 25}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if LangSmith credentials are set\n",
    "    if not os.environ.get(\"LANGCHAIN_API_KEY\"):\n",
    "        print(\"Warning: LANGCHAIN_API_KEY not set. LangSmith tracing will not work.\")\n",
    "        \n",
    "    # Set up the RAG system with tracing\n",
    "    qa_chain = setup_rag_system(collection_name=\"steam_reviews\", k=25)\n",
    "    \n",
    "    # Run a query with detailed performance tracing\n",
    "    query = \"What is the most common complaint about Counter-Strike Global Offensive in comparison to older Counter-Strike games?\"\n",
    "    result, metrics = run_rag_query(qa_chain, query)\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Result: {result['result']}\")\n",
    "    print(f\"Metrics: {metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
