{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading function definition and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_historical = f'{kagglehub.dataset_download(\"filipkin/steam-reviews\")}/output.csv'\n",
    "path_games = f'{kagglehub.dataset_download(\"filipkin/steam-reviews\")}/output_steamspy.csv'  \n",
    "path_latest = f'{kagglehub.dataset_download(\"tarasivaniv/steam-user-reviews-preprocessed-january-data\")}/preprocessed_validationset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = pd.read_csv(path_games)\n",
    "games.drop(columns=['appid', 'owners'], inplace=True)\n",
    "games.to_csv('games.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path_historical, path_latest, path_games):\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    ### Function to load data from csv files. Files can either be downloaed externally \n",
    "    # from kagglehub or already downloaded and stored in the same directory as the script.\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "\n",
    "    reviews_historical = pd.read_csv(path_historical)\n",
    "    reviews_latest = pd.read_csv(path_latest)\n",
    "    games = pd.read_csv(path_games)\n",
    "    reviews_all = pd.concat([reviews_historical, reviews_latest])\n",
    "    reviews_all = reviews_all[['id', 'app_id', 'content', 'author_id', 'is_positive']]\n",
    "    combined_data = pd.merge(reviews_all, games,\n",
    "                   left_on='app_id',\n",
    "                   right_on='appid',\n",
    "                   how='inner')\n",
    "    combined_data.dropna(inplace=True)\n",
    "    combined_data.drop_duplicates(inplace=True)\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>app_id</th>\n",
       "      <th>content</th>\n",
       "      <th>author_id</th>\n",
       "      <th>is_positive</th>\n",
       "      <th>appid</th>\n",
       "      <th>name</th>\n",
       "      <th>owners</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181398278</td>\n",
       "      <td>630</td>\n",
       "      <td>my friend fucking glitched out of the map. dud...</td>\n",
       "      <td>76561199715566328</td>\n",
       "      <td>Negative</td>\n",
       "      <td>630</td>\n",
       "      <td>Alien Swarm</td>\n",
       "      <td>2,000,000 .. 5,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179764650</td>\n",
       "      <td>630</td>\n",
       "      <td>mid</td>\n",
       "      <td>76561199175172339</td>\n",
       "      <td>Negative</td>\n",
       "      <td>630</td>\n",
       "      <td>Alien Swarm</td>\n",
       "      <td>2,000,000 .. 5,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>177801117</td>\n",
       "      <td>630</td>\n",
       "      <td>its just ok, valve games are not usually just ok</td>\n",
       "      <td>76561199197172151</td>\n",
       "      <td>Negative</td>\n",
       "      <td>630</td>\n",
       "      <td>Alien Swarm</td>\n",
       "      <td>2,000,000 .. 5,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>175959006</td>\n",
       "      <td>630</td>\n",
       "      <td>nobody is playing this game</td>\n",
       "      <td>76561198880505482</td>\n",
       "      <td>Negative</td>\n",
       "      <td>630</td>\n",
       "      <td>Alien Swarm</td>\n",
       "      <td>2,000,000 .. 5,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175942905</td>\n",
       "      <td>630</td>\n",
       "      <td>this game isn't very fun it runs like ass and ...</td>\n",
       "      <td>76561199208923353</td>\n",
       "      <td>Negative</td>\n",
       "      <td>630</td>\n",
       "      <td>Alien Swarm</td>\n",
       "      <td>2,000,000 .. 5,000,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  app_id                                            content  \\\n",
       "0  181398278     630  my friend fucking glitched out of the map. dud...   \n",
       "1  179764650     630                                                mid   \n",
       "2  177801117     630   its just ok, valve games are not usually just ok   \n",
       "3  175959006     630                        nobody is playing this game   \n",
       "4  175942905     630  this game isn't very fun it runs like ass and ...   \n",
       "\n",
       "           author_id is_positive  appid         name                  owners  \n",
       "0  76561199715566328    Negative    630  Alien Swarm  2,000,000 .. 5,000,000  \n",
       "1  76561199175172339    Negative    630  Alien Swarm  2,000,000 .. 5,000,000  \n",
       "2  76561199197172151    Negative    630  Alien Swarm  2,000,000 .. 5,000,000  \n",
       "3  76561198880505482    Negative    630  Alien Swarm  2,000,000 .. 5,000,000  \n",
       "4  76561199208923353    Negative    630  Alien Swarm  2,000,000 .. 5,000,000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data = load_data(path_historical, path_latest, path_games)\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preprocessing function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import contractions\n",
    "import re\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def preprocess_data(combined_data, rare_threshold=5, min_doc_freq=3, max_doc_freq_ratio=0.10, \n",
    "                   batch_size=None, n_process=None):\n",
    "    \"\"\"\n",
    "    Preprocesses text data from Steam reviews through multiple cleaning and filtering steps.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    combined_data : pandas.DataFrame\n",
    "        DataFrame containing the text data to process\n",
    "    rare_threshold : int\n",
    "        Minimum frequency for a token to not be considered rare\n",
    "    min_doc_freq : int\n",
    "        Minimum number of documents a term must appear in\n",
    "    max_doc_freq_ratio : float\n",
    "        Maximum percentage of documents a term can appear in\n",
    "    batch_size : int or None\n",
    "        Custom batch size for spaCy processing (calculated automatically if None)\n",
    "    n_process : int or None\n",
    "        Number of processes for parallel processing (calculated automatically if None)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with original and processed text data\n",
    "    \"\"\"\n",
    "    # Data validation and cleanup\n",
    "    if combined_data.isnull().any().any():\n",
    "        print(\"Missing values exist in the dataset.\")\n",
    "        combined_data = combined_data.dropna().copy()\n",
    "        print(f\"Missing values removed. Remaining rows: {len(combined_data)}\")\n",
    "    \n",
    "    if combined_data.duplicated().any():\n",
    "        print(\"Duplicates exist in the dataset.\")\n",
    "        combined_data = combined_data.drop_duplicates().copy()\n",
    "        print(f\"Duplicates removed. Remaining rows: {len(combined_data)}\")\n",
    "    else:\n",
    "        print(\"No duplicates or missing values exist in the dataset.\")\n",
    "        combined_data = combined_data.copy()  # Create a copy to avoid modifying original\n",
    "\n",
    "    # Basic text cleanup function\n",
    "    def basic_cleanup(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = contractions.fix(text)\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    print(\"Applying basic text cleanup...\")\n",
    "    combined_data['cleaned_text'] = [basic_cleanup(text) for text in tqdm(\n",
    "        combined_data['content'],\n",
    "        desc=\"Basic cleanup\",\n",
    "        unit=\"texts\"\n",
    "    )]\n",
    "    \n",
    "    # Configure spaCy processing\n",
    "    print(\"Loading spaCy model...\")\n",
    "    nlp = spacy.load('en_core_web_sm', \n",
    "                    disable=['ner', 'textcat', 'entity_linker', \n",
    "                            'entity_ruler', 'sentencizer'])\n",
    "    \n",
    "    # Optimize batch size based on average text length\n",
    "    if batch_size is None:\n",
    "        avg_text_length = combined_data['cleaned_text'].str.len().mean()\n",
    "        batch_size = max(100, int(1000000 / avg_text_length))\n",
    "    \n",
    "    if n_process is None:\n",
    "        n_process = min(cpu_count() - 1, 4)  # Leave some cores for system\n",
    "    \n",
    "    print(f\"Processing texts with batch size {batch_size} and {n_process} processes...\")\n",
    "    \n",
    "    # Process in chunks with improved progress tracking\n",
    "    chunk_size = 10000\n",
    "    total_chunks = (len(combined_data) // chunk_size) + (1 if len(combined_data) % chunk_size > 0 else 0)\n",
    "    all_tokens = []\n",
    "    \n",
    "    for i in range(0, len(combined_data), chunk_size):\n",
    "        chunk_end = min(i + chunk_size, len(combined_data))\n",
    "        print(f\"Processing chunk {i//chunk_size + 1}/{total_chunks} ({chunk_end - i} texts)\")\n",
    "        \n",
    "        texts = combined_data.iloc[i:chunk_end]['cleaned_text'].tolist()\n",
    "        chunk_tokens = []\n",
    "        \n",
    "        # Process the chunk with proper progress tracking\n",
    "        for doc in tqdm(nlp.pipe(texts, batch_size=batch_size, n_process=n_process), \n",
    "                        total=len(texts), desc=f\"Chunk {i//chunk_size + 1}\"):\n",
    "            try:\n",
    "                # Extract lemmas excluding stopwords, punctuation, and numbers\n",
    "                tokens = [token.lemma_ for token in doc \n",
    "                         if not token.is_stop and not token.is_punct \n",
    "                         and not token.like_num and len(token.lemma_) > 1]\n",
    "                chunk_tokens.append(tokens)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing document: {e}\")\n",
    "                chunk_tokens.append([])\n",
    "        \n",
    "        all_tokens.extend(chunk_tokens)\n",
    "        print(f\"Completed chunk {i//chunk_size + 1}/{total_chunks}\")\n",
    "    \n",
    "    # Add tokens to DataFrame\n",
    "    combined_data['tokens'] = all_tokens\n",
    "    \n",
    "    # Handle empty token lists\n",
    "    empty_tokens = combined_data['tokens'].apply(lambda x: len(x) == 0)\n",
    "    if empty_tokens.any():\n",
    "        print(f\"Removing {empty_tokens.sum()} rows with failed processing...\")\n",
    "        combined_data = combined_data[~empty_tokens].copy()\n",
    "    \n",
    "    # Calculate token frequencies more efficiently\n",
    "    print(\"Calculating token frequencies...\")\n",
    "    token_counter = Counter()\n",
    "    for tokens in tqdm(combined_data['tokens'], desc=\"Counting tokens\"):\n",
    "        token_counter.update(tokens)\n",
    "    \n",
    "    # Identify rare words\n",
    "    rare_words = {word for word, freq in token_counter.items() if freq < rare_threshold}\n",
    "    \n",
    "    # Calculate document frequencies directly from tokens\n",
    "    print(\"Calculating document frequencies...\")\n",
    "    doc_freq = Counter()\n",
    "    for tokens in tqdm(combined_data['tokens'], desc=\"Counting doc freqs\"):\n",
    "        # Count each token only once per document\n",
    "        doc_freq.update(set(tokens))\n",
    "    \n",
    "    # Filter by document frequency\n",
    "    num_docs = len(combined_data)\n",
    "    min_docs = min_doc_freq\n",
    "    max_docs = max_doc_freq_ratio * num_docs\n",
    "    \n",
    "    valid_words = {word for word, freq in doc_freq.items() \n",
    "                  if min_docs <= freq <= max_docs and word not in rare_words}\n",
    "    \n",
    "    # Apply final filtering\n",
    "    print(\"Applying final token filtering...\")\n",
    "    combined_data['filtered_tokens'] = combined_data['tokens'].apply(\n",
    "        lambda tokens: [t for t in tokens if t in valid_words]\n",
    "    )\n",
    "    \n",
    "    # Create filtered text from filtered tokens\n",
    "    combined_data['filtered_text'] = combined_data['filtered_tokens'].apply(\n",
    "        lambda tokens: \" \".join(tokens)\n",
    "    )\n",
    "    \n",
    "    # Print statistics\n",
    "    token_lengths = combined_data['filtered_tokens'].apply(len)\n",
    "    print(\"\\nProcessing Statistics:\")\n",
    "    print(f\"Final dataset shape: {combined_data.shape}\")\n",
    "    print(f\"Average tokens per text: {token_lengths.mean():.2f}\")\n",
    "    print(f\"Min tokens: {token_lengths.min()}\")\n",
    "    print(f\"Max tokens: {token_lengths.max()}\")\n",
    "    \n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates or missing values exist in the dataset.\n",
      "Applying basic text cleanup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Basic cleanup: 100%|██████████| 191655/191655 [00:00<00:00, 208370.31texts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy model...\n",
      "Processing texts with batch size 7737 and 4 processes...\n",
      "Processing chunk 1/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1: 100%|██████████| 10000/10000 [00:24<00:00, 405.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 1/20\n",
      "Processing chunk 2/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 2: 100%|██████████| 10000/10000 [00:23<00:00, 423.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 2/20\n",
      "Processing chunk 3/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 3: 100%|██████████| 10000/10000 [00:28<00:00, 349.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 3/20\n",
      "Processing chunk 4/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 4: 100%|██████████| 10000/10000 [00:18<00:00, 542.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 4/20\n",
      "Processing chunk 5/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 5: 100%|██████████| 10000/10000 [00:16<00:00, 607.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 5/20\n",
      "Processing chunk 6/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 6: 100%|██████████| 10000/10000 [00:16<00:00, 603.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 6/20\n",
      "Processing chunk 7/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 7: 100%|██████████| 10000/10000 [00:18<00:00, 555.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 7/20\n",
      "Processing chunk 8/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 8: 100%|██████████| 10000/10000 [00:19<00:00, 500.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 8/20\n",
      "Processing chunk 9/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 9: 100%|██████████| 10000/10000 [00:18<00:00, 529.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 9/20\n",
      "Processing chunk 10/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 10: 100%|██████████| 10000/10000 [00:14<00:00, 674.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 10/20\n",
      "Processing chunk 11/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 11: 100%|██████████| 10000/10000 [00:12<00:00, 808.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 11/20\n",
      "Processing chunk 12/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 12: 100%|██████████| 10000/10000 [00:14<00:00, 711.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 12/20\n",
      "Processing chunk 13/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 13: 100%|██████████| 10000/10000 [00:12<00:00, 779.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 13/20\n",
      "Processing chunk 14/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 14: 100%|██████████| 10000/10000 [00:14<00:00, 679.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 14/20\n",
      "Processing chunk 15/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 15: 100%|██████████| 10000/10000 [00:13<00:00, 752.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 15/20\n",
      "Processing chunk 16/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 16: 100%|██████████| 10000/10000 [00:11<00:00, 885.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 16/20\n",
      "Processing chunk 17/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 17: 100%|██████████| 10000/10000 [00:17<00:00, 556.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 17/20\n",
      "Processing chunk 18/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 18: 100%|██████████| 10000/10000 [00:16<00:00, 616.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 18/20\n",
      "Processing chunk 19/20 (10000 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 19: 100%|██████████| 10000/10000 [00:16<00:00, 622.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 19/20\n",
      "Processing chunk 20/20 (1655 texts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 20: 100%|██████████| 1655/1655 [00:05<00:00, 276.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed chunk 20/20\n",
      "Removing 7618 rows with failed processing...\n",
      "Calculating token frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting tokens: 100%|██████████| 184037/184037 [00:00<00:00, 999446.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating document frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting doc freqs: 100%|██████████| 184037/184037 [00:00<00:00, 836691.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying final token filtering...\n",
      "\n",
      "Processing Statistics:\n",
      "Final dataset shape: (184037, 12)\n",
      "Average tokens per text: 10.35\n",
      "Min tokens: 0\n",
      "Max tokens: 2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>app_id</th>\n",
       "      <th>content</th>\n",
       "      <th>author_id</th>\n",
       "      <th>is_positive</th>\n",
       "      <th>appid</th>\n",
       "      <th>name</th>\n",
       "      <th>owners</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>filtered_tokens</th>\n",
       "      <th>filtered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181398278</td>\n",
       "      <td>630</td>\n",
       "      <td>my friend fucking glitched out of the map. dud...</td>\n",
       "      <td>76561199715566328</td>\n",
       "      <td>Negative</td>\n",
       "      <td>630</td>\n",
       "      <td>Alien Swarm</td>\n",
       "      <td>2,000,000 .. 5,000,000</td>\n",
       "      <td>my friend fucking glitched out of the map dude...</td>\n",
       "      <td>[friend, fucking, glitche, map, dude]</td>\n",
       "      <td>[friend, fucking, glitche, map, dude]</td>\n",
       "      <td>friend fucking glitche map dude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179764650</td>\n",
       "      <td>630</td>\n",
       "      <td>mid</td>\n",
       "      <td>76561199175172339</td>\n",
       "      <td>Negative</td>\n",
       "      <td>630</td>\n",
       "      <td>Alien Swarm</td>\n",
       "      <td>2,000,000 .. 5,000,000</td>\n",
       "      <td>mid</td>\n",
       "      <td>[mid]</td>\n",
       "      <td>[mid]</td>\n",
       "      <td>mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>177801117</td>\n",
       "      <td>630</td>\n",
       "      <td>its just ok, valve games are not usually just ok</td>\n",
       "      <td>76561199197172151</td>\n",
       "      <td>Negative</td>\n",
       "      <td>630</td>\n",
       "      <td>Alien Swarm</td>\n",
       "      <td>2,000,000 .. 5,000,000</td>\n",
       "      <td>its just ok valve games are not usually just ok</td>\n",
       "      <td>[ok, valve, game, usually, ok]</td>\n",
       "      <td>[ok, valve, usually, ok]</td>\n",
       "      <td>ok valve usually ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>175959006</td>\n",
       "      <td>630</td>\n",
       "      <td>nobody is playing this game</td>\n",
       "      <td>76561198880505482</td>\n",
       "      <td>Negative</td>\n",
       "      <td>630</td>\n",
       "      <td>Alien Swarm</td>\n",
       "      <td>2,000,000 .. 5,000,000</td>\n",
       "      <td>nobody is playing this game</td>\n",
       "      <td>[play, game]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175942905</td>\n",
       "      <td>630</td>\n",
       "      <td>this game isn't very fun it runs like ass and ...</td>\n",
       "      <td>76561199208923353</td>\n",
       "      <td>Negative</td>\n",
       "      <td>630</td>\n",
       "      <td>Alien Swarm</td>\n",
       "      <td>2,000,000 .. 5,000,000</td>\n",
       "      <td>this game is not very fun it runs like ass and...</td>\n",
       "      <td>[game, fun, run, like, ass, community, ready, ...</td>\n",
       "      <td>[fun, run, like, ass, community, ready, kick, ...</td>\n",
       "      <td>fun run like ass community ready kick soon mis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  app_id                                            content  \\\n",
       "0  181398278     630  my friend fucking glitched out of the map. dud...   \n",
       "1  179764650     630                                                mid   \n",
       "2  177801117     630   its just ok, valve games are not usually just ok   \n",
       "3  175959006     630                        nobody is playing this game   \n",
       "4  175942905     630  this game isn't very fun it runs like ass and ...   \n",
       "\n",
       "           author_id is_positive  appid         name                  owners  \\\n",
       "0  76561199715566328    Negative    630  Alien Swarm  2,000,000 .. 5,000,000   \n",
       "1  76561199175172339    Negative    630  Alien Swarm  2,000,000 .. 5,000,000   \n",
       "2  76561199197172151    Negative    630  Alien Swarm  2,000,000 .. 5,000,000   \n",
       "3  76561198880505482    Negative    630  Alien Swarm  2,000,000 .. 5,000,000   \n",
       "4  76561199208923353    Negative    630  Alien Swarm  2,000,000 .. 5,000,000   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  my friend fucking glitched out of the map dude...   \n",
       "1                                                mid   \n",
       "2    its just ok valve games are not usually just ok   \n",
       "3                        nobody is playing this game   \n",
       "4  this game is not very fun it runs like ass and...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0              [friend, fucking, glitche, map, dude]   \n",
       "1                                              [mid]   \n",
       "2                     [ok, valve, game, usually, ok]   \n",
       "3                                       [play, game]   \n",
       "4  [game, fun, run, like, ass, community, ready, ...   \n",
       "\n",
       "                                     filtered_tokens  \\\n",
       "0              [friend, fucking, glitche, map, dude]   \n",
       "1                                              [mid]   \n",
       "2                           [ok, valve, usually, ok]   \n",
       "3                                                 []   \n",
       "4  [fun, run, like, ass, community, ready, kick, ...   \n",
       "\n",
       "                                       filtered_text  \n",
       "0                    friend fucking glitche map dude  \n",
       "1                                                mid  \n",
       "2                                ok valve usually ok  \n",
       "3                                                     \n",
       "4  fun run like ass community ready kick soon mis...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = preprocess_data(combined_data)\n",
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_reviews_for_rag(\n",
    "    dataframe: pd.DataFrame,\n",
    "    collection_name: str = \"steam_reviews\",\n",
    "    openai_api_key: str = None,\n",
    "    embedding_model: str = \"text-embedding-3-small\",\n",
    "    chunk_size: int = 1000,\n",
    "    batch_size: int = 100\n",
    ") -> Qdrant:\n",
    "    \"\"\"\n",
    "    Process Steam reviews and store them in Qdrant for use in a langchain RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: DataFrame containing Steam reviews\n",
    "        collection_name: Name for the Qdrant collection\n",
    "        openai_api_key: OpenAI API key (defaults to env variable)\n",
    "        embedding_model: OpenAI embedding model to use\n",
    "        chunk_size: Character length for text splitting (if needed)\n",
    "        batch_size: Batch size for processing\n",
    "        \n",
    "    Returns:\n",
    "        langchain.vectorstores.Qdrant: Configured Qdrant vector store for langchain\n",
    "    \"\"\"\n",
    "    # Set up API key\n",
    "    if openai_api_key is None:\n",
    "        openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "        if not openai_api_key:\n",
    "            raise ValueError(\"OpenAI API key required - provide as parameter or set OPENAI_API_KEY env variable\")\n",
    "    \n",
    "    # Initialize langchain's OpenAI embeddings\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=embedding_model,\n",
    "        openai_api_key=openai_api_key\n",
    "    )\n",
    "    \n",
    "    # Initialize Qdrant client\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "    \n",
    "    # Create or get collection\n",
    "    vector_size = 1536  # Size for text-embedding-3-small\n",
    "    try:\n",
    "        client.get_collection(collection_name)\n",
    "        print(f\"Collection '{collection_name}' already exists\")\n",
    "    except Exception:\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=models.VectorParams(\n",
    "                size=vector_size,\n",
    "                distance=models.Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "        print(f\"Created new collection '{collection_name}'\")\n",
    "    \n",
    "    # Create langchain documents from dataframe\n",
    "    documents = []\n",
    "    \n",
    "    print(f\"Processing {len(dataframe)} reviews...\")\n",
    "    for _, row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n",
    "        # Skip if filtered_text is missing\n",
    "        if pd.isna(row['filtered_text']) or row['filtered_text'] == \"\":\n",
    "            continue\n",
    "            \n",
    "        # Create a langchain Document with appropriate metadata\n",
    "        doc = Document(\n",
    "            page_content=row['filtered_text'],\n",
    "            metadata={\n",
    "                \"game_name\": row['name'],\n",
    "                \"review_id\": str(row.name),  # Using index as review_id\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    print(f\"Created {len(documents)} langchain documents\")\n",
    "    \n",
    "    # Optional: Split long reviews into smaller chunks for better retrieval\n",
    "    # Only use this if your reviews are very long\n",
    "    if chunk_size > 0:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=50,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        documents = text_splitter.split_documents(documents)\n",
    "        print(f\"Split into {len(documents)} chunks\")\n",
    "    \n",
    "    # Create Qdrant vector store using langchain's integration\n",
    "    print(\"Storing documents in Qdrant...\")\n",
    "    vector_store = Qdrant.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        collection_name=collection_name,\n",
    "        url=\"http://localhost:6333\",\n",
    "        prefer_grpc=False,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully stored {len(documents)} documents in Qdrant\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/gyqgx68n4v72zw8nb1r3jrj80000gn/T/ipykernel_24017/3734454712.py:30: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new collection 'steam_reviews'\n",
      "Processing 184037 reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184037/184037 [00:02<00:00, 72838.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 173367 langchain documents\n",
      "Split into 172576 chunks\n",
      "Storing documents in Qdrant...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m rag_store = \u001b[43mstore_reviews_for_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mstore_reviews_for_rag\u001b[39m\u001b[34m(dataframe, collection_name, openai_api_key, embedding_model, chunk_size, batch_size)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Create Qdrant vector store using langchain's integration\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStoring documents in Qdrant...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m vector_store = \u001b[43mQdrant\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp://localhost:6333\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer_grpc\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccessfully stored \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents in Qdrant\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m vector_store\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:847\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    844\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    845\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m847\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/langchain_community/vectorstores/qdrant.py:1369\u001b[39m, in \u001b[36mQdrant.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, location, url, port, grpc_port, prefer_grpc, https, api_key, prefix, timeout, host, path, collection_name, distance_func, content_payload_key, metadata_payload_key, vector_name, batch_size, shard_number, replication_factor, write_consistency_factor, on_disk_payload, hnsw_config, optimizers_config, wal_config, quantization_config, init_from, on_disk, force_recreate, **kwargs)\u001b[39m\n\u001b[32m   1234\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Construct Qdrant wrapper from a list of texts.\u001b[39;00m\n\u001b[32m   1235\u001b[39m \n\u001b[32m   1236\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1335\u001b[39m \u001b[33;03m        qdrant = Qdrant.from_texts(texts, embeddings, \"localhost\")\u001b[39;00m\n\u001b[32m   1336\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1337\u001b[39m qdrant = \u001b[38;5;28mcls\u001b[39m.construct_instance(\n\u001b[32m   1338\u001b[39m     texts,\n\u001b[32m   1339\u001b[39m     embedding,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1367\u001b[39m     **kwargs,\n\u001b[32m   1368\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m \u001b[43mqdrant\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m qdrant\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/langchain_community/vectorstores/qdrant.py:189\u001b[39m, in \u001b[36mQdrant.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, batch_size, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run more texts through the embeddings and add to the vectorstore.\u001b[39;00m\n\u001b[32m    174\u001b[39m \n\u001b[32m    175\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    186\u001b[39m \u001b[33;03m    List of ids from adding the texts into the vectorstore.\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    188\u001b[39m added_ids = []\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_rest_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43madded_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/langchain_community/vectorstores/qdrant.py:2165\u001b[39m, in \u001b[36mQdrant._generate_rest_batches\u001b[39m\u001b[34m(self, texts, metadatas, ids, batch_size)\u001b[39m\n\u001b[32m   2162\u001b[39m batch_ids = \u001b[38;5;28mlist\u001b[39m(islice(ids_iterator, batch_size))\n\u001b[32m   2164\u001b[39m \u001b[38;5;66;03m# Generate the embeddings for all the texts in a batch\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2165\u001b[39m batch_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2167\u001b[39m points = [\n\u001b[32m   2168\u001b[39m     rest.PointStruct(\n\u001b[32m   2169\u001b[39m         \u001b[38;5;28mid\u001b[39m=point_id,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2184\u001b[39m     )\n\u001b[32m   2185\u001b[39m ]\n\u001b[32m   2187\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m batch_ids, points\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/langchain_community/vectorstores/qdrant.py:2105\u001b[39m, in \u001b[36mQdrant._embed_texts\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m   2094\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Embed search texts.\u001b[39;00m\n\u001b[32m   2095\u001b[39m \n\u001b[32m   2096\u001b[39m \u001b[33;03mUsed to provide backward compatibility with `embedding_function` argument.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2102\u001b[39m \u001b[33;03m    List of floats representing the texts embedding.\u001b[39;00m\n\u001b[32m   2103\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2105\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(embeddings, \u001b[33m\"\u001b[39m\u001b[33mtolist\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2107\u001b[39m         embeddings = embeddings.tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/langchain_community/embeddings/openai.py:671\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size)\u001b[39m\n\u001b[32m    668\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    669\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m    670\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/langchain_community/embeddings/openai.py:497\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size)\u001b[39m\n\u001b[32m    495\u001b[39m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m     response = \u001b[43membed_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    503\u001b[39m         response = response.dict()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/langchain_community/embeddings/openai.py:120\u001b[39m, in \u001b[36membed_with_retry\u001b[39m\u001b[34m(embeddings, **kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m retry_decorator = _create_retry_decorator(embeddings)\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_embed_with_retry\u001b[39m(**kwargs: Any) -> Any:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/openai/resources/embeddings.py:128\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    122\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    123\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m             ).tolist()\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/openai/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/openai/_base_client.py:919\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    917\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/openai/_base_client.py:955\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m    952\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, request.method, request.url)\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    961\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/WDisk/Coding/NLP langchain - Steam user Reviews/venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "rag_store = store_reviews_for_rag(preprocessed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
